{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from scipy.misc import imread\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the images and annotations path\n",
    "base_dataset_dir = \"/home/thalles_silva/DataPublic/Road_and_Buildings_detection_dataset/mass_merged\"\n",
    "train_dataset_base_dir = os.path.join(base_dataset_dir, \"train\")\n",
    "images_folder_name = \"sat/\"\n",
    "annotations_folder_name = \"map/\"\n",
    "train_images_dir = os.path.join(train_dataset_base_dir, images_folder_name)\n",
    "train_annotations_dir = os.path.join(train_dataset_base_dir, annotations_folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_train_examples: 137\n"
     ]
    }
   ],
   "source": [
    "# read the train.txt file. This file contains the training images' names\n",
    "file = open(os.path.join(train_dataset_base_dir, \"train_all.txt\"), 'r')\n",
    "images_filename_list = [line for line in file]\n",
    "number_of_train_examples = len(images_filename_list)\n",
    "print(\"number_of_train_examples:\", number_of_train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the images and annotations path\n",
    "val_dataset_base_dir = os.path.join(base_dataset_dir, \"valid\")\n",
    "val_images_dir = os.path.join(val_dataset_base_dir, images_folder_name)\n",
    "val_annotations_dir = os.path.join(val_dataset_base_dir, annotations_folder_name)\n",
    "\n",
    "# read the train.txt file. This file contains the training images' names\n",
    "file = open(os.path.join(val_dataset_base_dir, \"val.txt\"), 'r')\n",
    "val_images_filename_list = [line for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATASET_DIR=\"../dataset/\"\n",
    "TRAIN_FILE = 'train.tfrecords'\n",
    "VALIDATION_FILE = 'validation.tfrecords'\n",
    "train_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,TRAIN_FILE))\n",
    "val_writer = tf.python_io.TFRecordWriter(os.path.join(TRAIN_DATASET_DIR,VALIDATION_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image_and_annotation(train_images_dir, train_annotations_dir, image_name):\n",
    "    # read the input and annotation images\n",
    "    image = imread(train_images_dir + image_name.strip() + \".tiff\")\n",
    "    annotation = imread(train_annotations_dir + image_name.strip() + \".tif\")\n",
    "\n",
    "    return image, annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_crop(image_np, annotation_np, crop_size=128):\n",
    "    \"\"\"\n",
    "    image_np: rgb image shape (H,W,3)\n",
    "    annotation_np: 1D image shape (H,W,1)\n",
    "    crop_size: integer\n",
    "    \"\"\"\n",
    "    image_h = image_np.shape[0]\n",
    "    image_w = image_np.shape[1]\n",
    "\n",
    "    random_x = np.random.randint(0, image_w-crop_size+1) # Return random integers from low (inclusive) to high (exclusive).\n",
    "    random_y = np.random.randint(0, image_h-crop_size+1) # Return random integers from low (inclusive) to high (exclusive).\n",
    "\n",
    "    offset_x = random_x + crop_size\n",
    "    offset_y = random_y + crop_size\n",
    "\n",
    "    return image_np[random_x:offset_x, random_y:offset_y,:], annotation_np[random_x:offset_x, random_y:offset_y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_tfrecord_dataset(images_dir, annotations_dir, filename_list, writer, total_epochs=1, batch_size=1, crop_size=64, random_cropping=True):\n",
    "\n",
    "    print(\"Total # of example:\", batch_size*len(filename_list)*total_epochs)\n",
    "    number_of_written_imgs = 0\n",
    "    for epoch_counter in range(total_epochs):\n",
    "        for image_name in filename_list:\n",
    "\n",
    "            image_np, annotation_np = read_image_and_annotation(images_dir, annotations_dir, image_name)\n",
    "  \n",
    "            for batch_i in range(batch_size):\n",
    "\n",
    "                if random_cropping:\n",
    "                    while True:\n",
    "                        image_np_cropped, annotation_np_cropped = random_crop(image_np, annotation_np, crop_size)\n",
    "                        \n",
    "                        total_n_of_pixels = crop_size*crop_size\n",
    "                        \n",
    "                        # count the number of background pixels in the annotation patch\n",
    "                        background_pixels = total_n_of_pixels - np.count_nonzero(annotation_np_cropped)\n",
    "                        \n",
    "                        prob = random.random()\n",
    "                        \n",
    "                        # if # of background pixels > 90% of total pixels, discard with prob of 50%\n",
    "                        if background_pixels >= 0.99 * total_n_of_pixels:\n",
    "                            if prob <= 0.8:\n",
    "                                #print(\"Discard image. 99% background\")\n",
    "                                continue\n",
    "                        elif background_pixels >= 0.95 * total_n_of_pixels:\n",
    "                            if prob <= 0.65:\n",
    "                                #print(\"Discard image. 95% background\")\n",
    "                                continue\n",
    "                        elif background_pixels >= 0.9 * total_n_of_pixels:\n",
    "                            if prob <= 0.5:\n",
    "                                #print(\"Discard image. 90% background\")\n",
    "                                continue\n",
    "\n",
    "                        # count the # of zeros in the image patch, because the dataset has some images with zeros (invalid areas)\n",
    "                        # we crop patches that have less than 10% of white pixels in it\n",
    "                        n_of_zeros = np.sum(np.all(image_np_cropped == [255,255,255], axis=2))\n",
    "                        \n",
    "                        #print(\"# of zeros:\", n_of_zeros, \"from image:\", image_name)\n",
    "                        if n_of_zeros < 0.01 * (crop_size * crop_size):\n",
    "                            break\n",
    "                else:\n",
    "                    batch_size = 1 # for negative random crop, never iterate over the same image\n",
    "                    image_np_cropped = image_np\n",
    "                    annotation_np_cropped = annotation_np\n",
    "                    \n",
    "                image_h = image_np_cropped.shape[0]\n",
    "                image_w = image_np_cropped.shape[1]\n",
    "          \n",
    "                img_raw = image_np_cropped.tostring()\n",
    "                annotation_raw = annotation_np_cropped.tostring()\n",
    "\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                        'height': _int64_feature(image_h),\n",
    "                        'width': _int64_feature(image_w),\n",
    "                        'image_raw': _bytes_feature(img_raw),\n",
    "                        'annotation_raw': _bytes_feature(annotation_raw)}))\n",
    "                \n",
    "                writer.write(example.SerializeToString())\n",
    "                number_of_written_imgs += 1\n",
    "                \n",
    "        print(\"Image written:\", number_of_written_imgs, \"End of epoch:\",epoch_counter)\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total # of example: 561152\n",
      "Image written: 8768 End of epoch: 0\n",
      "Image written: 17536 End of epoch: 1\n",
      "Image written: 26304 End of epoch: 2\n",
      "Image written: 35072 End of epoch: 3\n",
      "Image written: 43840 End of epoch: 4\n",
      "Image written: 52608 End of epoch: 5\n",
      "Image written: 61376 End of epoch: 6\n",
      "Image written: 70144 End of epoch: 7\n",
      "Image written: 78912 End of epoch: 8\n",
      "Image written: 87680 End of epoch: 9\n",
      "Image written: 96448 End of epoch: 10\n",
      "Image written: 105216 End of epoch: 11\n",
      "Image written: 113984 End of epoch: 12\n",
      "Image written: 122752 End of epoch: 13\n",
      "Image written: 131520 End of epoch: 14\n",
      "Image written: 140288 End of epoch: 15\n",
      "Image written: 149056 End of epoch: 16\n",
      "Image written: 157824 End of epoch: 17\n",
      "Image written: 166592 End of epoch: 18\n",
      "Image written: 175360 End of epoch: 19\n",
      "Image written: 184128 End of epoch: 20\n",
      "Image written: 192896 End of epoch: 21\n",
      "Image written: 201664 End of epoch: 22\n",
      "Image written: 210432 End of epoch: 23\n",
      "Image written: 219200 End of epoch: 24\n",
      "Image written: 227968 End of epoch: 25\n",
      "Image written: 236736 End of epoch: 26\n",
      "Image written: 245504 End of epoch: 27\n",
      "Image written: 254272 End of epoch: 28\n",
      "Image written: 263040 End of epoch: 29\n",
      "Image written: 271808 End of epoch: 30\n",
      "Image written: 280576 End of epoch: 31\n",
      "Image written: 289344 End of epoch: 32\n",
      "Image written: 298112 End of epoch: 33\n",
      "Image written: 306880 End of epoch: 34\n",
      "Image written: 315648 End of epoch: 35\n",
      "Image written: 324416 End of epoch: 36\n",
      "Image written: 333184 End of epoch: 37\n",
      "Image written: 341952 End of epoch: 38\n",
      "Image written: 350720 End of epoch: 39\n",
      "Image written: 359488 End of epoch: 40\n",
      "Image written: 368256 End of epoch: 41\n",
      "Image written: 377024 End of epoch: 42\n",
      "Image written: 385792 End of epoch: 43\n",
      "Image written: 394560 End of epoch: 44\n",
      "Image written: 403328 End of epoch: 45\n",
      "Image written: 412096 End of epoch: 46\n",
      "Image written: 420864 End of epoch: 47\n",
      "Image written: 429632 End of epoch: 48\n",
      "Image written: 438400 End of epoch: 49\n",
      "Image written: 447168 End of epoch: 50\n",
      "Image written: 455936 End of epoch: 51\n",
      "Image written: 464704 End of epoch: 52\n",
      "Image written: 473472 End of epoch: 53\n",
      "Image written: 482240 End of epoch: 54\n",
      "Image written: 491008 End of epoch: 55\n",
      "Image written: 499776 End of epoch: 56\n",
      "Image written: 508544 End of epoch: 57\n",
      "Image written: 517312 End of epoch: 58\n",
      "Image written: 526080 End of epoch: 59\n",
      "Image written: 534848 End of epoch: 60\n",
      "Image written: 543616 End of epoch: 61\n",
      "Image written: 552384 End of epoch: 62\n",
      "Image written: 561152 End of epoch: 63\n",
      "Total # of example: 4\n",
      "Image written: 4 End of epoch: 0\n"
     ]
    }
   ],
   "source": [
    "create_tfrecord_dataset(train_images_dir, train_annotations_dir, images_filename_list, train_writer, crop_size=64, batch_size=64, total_epochs=64)\n",
    "create_tfrecord_dataset(val_images_dir, val_annotations_dir, val_images_filename_list, val_writer, random_cropping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py3)",
   "language": "python",
   "name": "py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
